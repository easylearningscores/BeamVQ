{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: Zongyi Li\n",
    "This file is the Fourier Neural Operator for 2D problem such as the Navier-Stokes equation discussed in Section 5.3 in the [paper](https://arxiv.org/pdf/2010.08895.pdf),\n",
    "which uses a recurrent structure to propagates in time.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "from functools import partial\n",
    "\n",
    "from timeit import default_timer\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "################################################################\n",
    "# fourier layer\n",
    "################################################################\n",
    "\n",
    "class SpectralConv2d_fast(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super(SpectralConv2d_fast, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        2D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "        self.modes2 = modes2\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul2d(self, input, weights):\n",
    "        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\n",
    "        return torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.rfft2(x)\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels,  x.size(-2), x.size(-1)//2 + 1, dtype=torch.cfloat, device=x.device)\n",
    "        out_ft[:, :, :self.modes1, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights1)\n",
    "        out_ft[:, :, -self.modes1:, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)\n",
    "\n",
    "        #Return to physical space\n",
    "        x = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))\n",
    "        return x\n",
    "\n",
    "class FNO2d(nn.Module):\n",
    "    def __init__(self, input_len, modes1, modes2, pred_len, width):\n",
    "        super(FNO2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the previous 10 timesteps + 2 locations (u(t-10, x, y), ..., u(t-1, x, y),  x, y)\n",
    "        input shape: (batchsize, x=64, y=64, c=12)\n",
    "        output: the solution of the next timestep\n",
    "        output shape: (batchsize, x=64, y=64, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.width = width\n",
    "        self.pred_len = pred_len\n",
    "        self.padding = 2 # pad the domain if input is non-periodic\n",
    "        self.input_len = input_len\n",
    "        inputs_dim = self.input_len + self.padding\n",
    "        self.fc0 = nn.Linear(inputs_dim, self.width)\n",
    "        # input channel is 12: the solution of the previous 10 timesteps + 2 locations (u(t-10, x, y), ..., u(t-1, x, y),  x, y)\n",
    "\n",
    "        self.conv0 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv1 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv2 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv3 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.w0 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.bn0 = torch.nn.BatchNorm2d(self.width)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(self.width)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(self.width)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(self.width)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 128)\n",
    "        self.fc2 = nn.Linear(128, pred_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # b, t, c, h, w = x.shape\n",
    "        # x = x.reshape(b, t*c, h, w) # b h w t\n",
    "        # x = x.permute(0, 2, 3, 1)\n",
    "        grid = self.get_grid(x.shape, x.device)\n",
    "        x = torch.cat((x, grid), dim=-1)\n",
    "        x = self.fc0(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        # x = F.pad(x, [0,self.padding, 0,self.padding]) # pad the domain if input is non-periodic\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv2(x)\n",
    "        x2 = self.w2(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv3(x)\n",
    "        x2 = self.w3(x)\n",
    "        x = x1 + x2\n",
    "\n",
    "        # x = x[..., :-self.padding, :-self.padding] # pad the domain if input is non-periodic\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x) # batch_size, h, w, t\n",
    "        # x = x.permute(0, 3, 1, 2)\n",
    "        # x = x.reshape(b, t, c, h, w)\n",
    "        return x\n",
    "\n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64, 10])\n"
     ]
    }
   ],
   "source": [
    "model = FNO2d(input_len = 10, modes1 = 12, modes2 = 12, pred_len = 10, width = 20)\n",
    "inputs = torch.rand(1, 64, 64,10)\n",
    "output = model(inputs)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "def load_navier_stokes_data(path, sub=1, T_in=10, T_out=10, batch_size=20, reshape=None):\n",
    "    ntrain = 1000\n",
    "    neval = 100\n",
    "    ntest = 100\n",
    "    total = ntrain + neval + ntest\n",
    "    f = scipy.io.loadmat(path)\n",
    "    data = f['u'][..., 0:total]\n",
    "    data = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "    # Training data\n",
    "    train_a = data[:ntrain, ::sub, ::sub, :T_in]\n",
    "    train_u = data[:ntrain, ::sub, ::sub, T_in:T_out+T_in] # [N, H,W,T]\n",
    "    # train_a = train_a.unsqueeze(-1).permute(0, 3, 1, 2, 4).permute(0, 1, 4, 2, 3)  # From [N, H, W, T] to [N, T, H, W, C]\n",
    "    # train_u = train_u.unsqueeze(-1).permute(0, 3, 1, 2, 4).permute(0, 1, 4, 2, 3)\n",
    "    #print(train_a.shape, train_u.shape)\n",
    "    # Evaluation data\n",
    "    eval_a = data[ntrain:ntrain + neval, ::sub, ::sub, :T_in]\n",
    "    eval_u = data[ntrain:ntrain + neval, ::sub, ::sub, T_in:T_out+T_in]\n",
    "    # eval_a = eval_a.unsqueeze(-1).permute(0, 3, 1, 2, 4).permute(0, 1, 4, 2, 3)  # From [N, H, W, T] to [N, T, H, W, C]\n",
    "    # eval_u = eval_u.unsqueeze(-1).permute(0, 3, 1, 2, 4).permute(0, 1, 4, 2, 3)\n",
    "    # Testing data\n",
    "    test_a = data[ntrain + neval:ntrain + neval + ntest, ::sub, ::sub, :T_in]\n",
    "    test_u = data[ntrain + neval:ntrain + neval + ntest, ::sub, ::sub, T_in:T_out+T_in]\n",
    "    # test_a = test_a.unsqueeze(-1).permute(0, 3, 1, 2, 4).permute(0, 1, 4, 2, 3)  # From [N, H, W, T] to [N, T, H, W, C] to [N, T, C, H, W]\n",
    "    # test_u = test_u.unsqueeze(-1).permute(0, 3, 1, 2, 4).permute(0, 1, 4, 2, 3)\n",
    "\n",
    "    if reshape:\n",
    "        train_a = train_a.permute(reshape)\n",
    "        train_u = train_u.permute(reshape)\n",
    "        eval_a = eval_a.permute(reshape)\n",
    "        eval_u = eval_u.permute(reshape)\n",
    "        test_a = test_a.permute(reshape)\n",
    "        test_u = test_u.permute(reshape)\n",
    "        \n",
    "    train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(train_a, train_u), batch_size=batch_size, shuffle=True)\n",
    "    eval_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(eval_a, eval_u), batch_size=batch_size, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(test_a, test_u), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, eval_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 64, 64, 10]) torch.Size([20, 64, 64, 10])\n"
     ]
    }
   ],
   "source": [
    "train_loader, eval_loader, test_loader = load_navier_stokes_data(path=\"/root/autodl-tmp/dataset/NavierStokes_V1e-5_N1200_T20.mat\")\n",
    "for inputs, targets in iter(train_loader):\n",
    "    print(inputs.shape, targets.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import math\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def FDM_NS_vorticity(w, v=1/40, t_interval=1.0):\n",
    "    batchsize = w.size(0)  # w.shape = b h w t\n",
    "    nx = w.size(1)\n",
    "    ny = w.size(2)\n",
    "    nt = w.size(3)\n",
    "    device = w.device\n",
    "    w = w.reshape(batchsize, nx, ny, nt)\n",
    "\n",
    "    w_h = torch.fft.fft2(w, dim=[1, 2])\n",
    "    k_max = nx // 2\n",
    "    N = nx\n",
    "    k_x = torch.cat((torch.arange(0, k_max, device=device), torch.arange(-k_max, 0, device=device)), 0).reshape(N, 1).repeat(1, N).reshape(1, N, N, 1)\n",
    "    k_y = torch.cat((torch.arange(0, k_max, device=device), torch.arange(-k_max, 0, device=device)), 0).reshape(1, N).repeat(N, 1).reshape(1, N, N, 1)\n",
    "\n",
    "    lap = (k_x ** 2 + k_y ** 2)\n",
    "    lap[0, 0, 0, 0] = 1.0\n",
    "    f_h = w_h / lap\n",
    "\n",
    "    ux_h = 1j * k_y * f_h\n",
    "    uy_h = -1j * k_x * f_h\n",
    "    wx_h = 1j * k_x * w_h\n",
    "    wy_h = 1j * k_y * w_h\n",
    "    wlap_h = -lap * w_h\n",
    "\n",
    "    ux = torch.fft.irfft2(ux_h[:, :, :k_max + 1], dim=[1, 2])\n",
    "    uy = torch.fft.irfft2(uy_h[:, :, :k_max + 1], dim=[1, 2])\n",
    "    wx = torch.fft.irfft2(wx_h[:, :, :k_max + 1], dim=[1, 2])\n",
    "    wy = torch.fft.irfft2(wy_h[:, :, :k_max + 1], dim=[1, 2])\n",
    "    wlap = torch.fft.irfft2(wlap_h[:, :, :k_max + 1], dim=[1, 2])\n",
    "\n",
    "    dt = t_interval / (nt - 1)\n",
    "    wt = (w[:, :, :, 2:] - w[:, :, :, :-2]) / (2 * dt)\n",
    "\n",
    "    Du1 = wt + (ux * wx + uy * wy - v * wlap)[..., 1:-1]\n",
    "    return Du1\n",
    "\n",
    "# LpLoss \n",
    "class LpLoss:\n",
    "    def __init__(self, size_average=True):\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def __call__(self, pred, target):\n",
    "        pred = pred.to(device)\n",
    "        target = target.to(device)\n",
    "        loss = torch.abs(pred - target)\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        return loss.sum()\n",
    "\n",
    "#  PINO_loss_2d \n",
    "def PINO_loss_2d(u, u0, forcing, v=1/40, t_interval=1.0):\n",
    "    batchsize = u.size(0)\n",
    "    nx = u.size(1)\n",
    "    ny = u.size(2)\n",
    "    nt = u.size(3)\n",
    "\n",
    "    u = u.reshape(batchsize, nx, ny, nt)\n",
    "    lploss = LpLoss(size_average=True)\n",
    "\n",
    "    u_in = u[:, :, :, 0].to(device)\n",
    "    u0 = u0.to(device)\n",
    "    loss_ic = lploss(u_in, u0)\n",
    "\n",
    "    Du = FDM_NS_vorticity(u, v, t_interval)\n",
    "    f = forcing.repeat(batchsize, 1, 1, 1)\n",
    "    loss_f = lploss(Du, f)\n",
    "\n",
    "    return loss_ic, loss_f\n",
    "\n",
    "\n",
    "s = 64\n",
    "t = torch.linspace(0, 1, s+1, device=device)\n",
    "t = t[0:-1]\n",
    "\n",
    "X, Y = torch.meshgrid(t, t)\n",
    "f = 0.1 * (torch.sin(2 * math.pi * (X + Y)) + torch.cos(2 * math.pi * (X + Y)))\n",
    "\n",
    "nt = 10\n",
    "forcing = f.unsqueeze(-1).repeat(1, 1, nt).to(device)\n",
    "\n",
    "\n",
    "model = FNO2d(input_len=10, modes1=12, modes2=12, pred_len=10, width=20).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, eval_loader, criterion, optimizer, num_epochs=100):\n",
    "    best_loss = np.inf\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            u0 = inputs[:, :, :, 0]\n",
    "            time_steps = inputs.shape[-1]\n",
    "            u = torch.zeros(inputs.shape[0], inputs.shape[1], inputs.shape[2], time_steps + 2, device=device)\n",
    "            u[:, :, :, :time_steps] = inputs\n",
    "            loss_ic, loss_f = PINO_loss_2d(u, u0, forcing, v=1/40, t_interval=1.0)\n",
    "\n",
    "            loss = criterion(outputs, targets) + loss_ic + loss_f\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        model.eval()\n",
    "        eval_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in eval_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                eval_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        eval_loss /= len(eval_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Eval Loss: {eval_loss:.4f}')\n",
    "\n",
    "        if eval_loss < best_loss:\n",
    "            best_loss = eval_loss\n",
    "            torch.save(model.state_dict(), 'fnoPINO_best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 3.2179, Eval Loss: 0.8256\n",
      "Epoch 2, Train Loss: 2.5167, Eval Loss: 0.5893\n",
      "Epoch 3, Train Loss: 2.2442, Eval Loss: 0.3789\n",
      "Epoch 4, Train Loss: 2.1553, Eval Loss: 0.3386\n",
      "Epoch 5, Train Loss: 2.1195, Eval Loss: 0.3054\n",
      "Epoch 6, Train Loss: 2.0909, Eval Loss: 0.2809\n",
      "Epoch 7, Train Loss: 2.0696, Eval Loss: 0.2625\n",
      "Epoch 8, Train Loss: 2.0528, Eval Loss: 0.2483\n",
      "Epoch 9, Train Loss: 2.0398, Eval Loss: 0.2376\n",
      "Epoch 10, Train Loss: 2.0301, Eval Loss: 0.2298\n",
      "Epoch 11, Train Loss: 2.0222, Eval Loss: 0.2215\n",
      "Epoch 12, Train Loss: 2.0149, Eval Loss: 0.2165\n",
      "Epoch 13, Train Loss: 2.0090, Eval Loss: 0.2126\n",
      "Epoch 14, Train Loss: 2.0039, Eval Loss: 0.2073\n",
      "Epoch 15, Train Loss: 1.9988, Eval Loss: 0.2025\n",
      "Epoch 16, Train Loss: 1.9940, Eval Loss: 0.1978\n",
      "Epoch 17, Train Loss: 1.9899, Eval Loss: 0.1944\n",
      "Epoch 18, Train Loss: 1.9864, Eval Loss: 0.1915\n",
      "Epoch 19, Train Loss: 1.9831, Eval Loss: 0.1884\n",
      "Epoch 20, Train Loss: 1.9792, Eval Loss: 0.1866\n",
      "Epoch 21, Train Loss: 1.9764, Eval Loss: 0.1847\n",
      "Epoch 22, Train Loss: 1.9733, Eval Loss: 0.1809\n",
      "Epoch 23, Train Loss: 1.9704, Eval Loss: 0.1790\n",
      "Epoch 24, Train Loss: 1.9689, Eval Loss: 0.1794\n",
      "Epoch 25, Train Loss: 1.9660, Eval Loss: 0.1759\n",
      "Epoch 26, Train Loss: 1.9639, Eval Loss: 0.1737\n",
      "Epoch 27, Train Loss: 1.9615, Eval Loss: 0.1733\n",
      "Epoch 28, Train Loss: 1.9597, Eval Loss: 0.1709\n",
      "Epoch 29, Train Loss: 1.9578, Eval Loss: 0.1700\n",
      "Epoch 30, Train Loss: 1.9559, Eval Loss: 0.1697\n",
      "Epoch 31, Train Loss: 1.9539, Eval Loss: 0.1665\n",
      "Epoch 32, Train Loss: 1.9525, Eval Loss: 0.1658\n",
      "Epoch 33, Train Loss: 1.9508, Eval Loss: 0.1650\n",
      "Epoch 34, Train Loss: 1.9490, Eval Loss: 0.1637\n",
      "Epoch 35, Train Loss: 1.9475, Eval Loss: 0.1630\n",
      "Epoch 36, Train Loss: 1.9464, Eval Loss: 0.1615\n",
      "Epoch 37, Train Loss: 1.9446, Eval Loss: 0.1615\n",
      "Epoch 38, Train Loss: 1.9432, Eval Loss: 0.1605\n",
      "Epoch 39, Train Loss: 1.9422, Eval Loss: 0.1598\n",
      "Epoch 40, Train Loss: 1.9410, Eval Loss: 0.1586\n",
      "Epoch 41, Train Loss: 1.9393, Eval Loss: 0.1576\n",
      "Epoch 42, Train Loss: 1.9383, Eval Loss: 0.1565\n",
      "Epoch 43, Train Loss: 1.9369, Eval Loss: 0.1555\n",
      "Epoch 44, Train Loss: 1.9362, Eval Loss: 0.1556\n",
      "Epoch 45, Train Loss: 1.9348, Eval Loss: 0.1544\n",
      "Epoch 46, Train Loss: 1.9337, Eval Loss: 0.1530\n",
      "Epoch 47, Train Loss: 1.9324, Eval Loss: 0.1539\n",
      "Epoch 48, Train Loss: 1.9317, Eval Loss: 0.1523\n",
      "Epoch 49, Train Loss: 1.9301, Eval Loss: 0.1515\n",
      "Epoch 50, Train Loss: 1.9292, Eval Loss: 0.1507\n",
      "Epoch 51, Train Loss: 1.9281, Eval Loss: 0.1507\n",
      "Epoch 52, Train Loss: 1.9272, Eval Loss: 0.1496\n",
      "Epoch 53, Train Loss: 1.9261, Eval Loss: 0.1494\n",
      "Epoch 54, Train Loss: 1.9253, Eval Loss: 0.1484\n",
      "Epoch 55, Train Loss: 1.9242, Eval Loss: 0.1485\n",
      "Epoch 56, Train Loss: 1.9235, Eval Loss: 0.1480\n",
      "Epoch 57, Train Loss: 1.9227, Eval Loss: 0.1470\n",
      "Epoch 58, Train Loss: 1.9219, Eval Loss: 0.1466\n",
      "Epoch 59, Train Loss: 1.9207, Eval Loss: 0.1462\n",
      "Epoch 60, Train Loss: 1.9198, Eval Loss: 0.1453\n",
      "Epoch 61, Train Loss: 1.9190, Eval Loss: 0.1451\n",
      "Epoch 62, Train Loss: 1.9182, Eval Loss: 0.1445\n",
      "Epoch 63, Train Loss: 1.9177, Eval Loss: 0.1444\n",
      "Epoch 64, Train Loss: 1.9167, Eval Loss: 0.1444\n",
      "Epoch 65, Train Loss: 1.9160, Eval Loss: 0.1441\n",
      "Epoch 66, Train Loss: 1.9152, Eval Loss: 0.1436\n",
      "Epoch 67, Train Loss: 1.9143, Eval Loss: 0.1428\n",
      "Epoch 68, Train Loss: 1.9134, Eval Loss: 0.1427\n",
      "Epoch 69, Train Loss: 1.9130, Eval Loss: 0.1417\n",
      "Epoch 70, Train Loss: 1.9122, Eval Loss: 0.1417\n",
      "Epoch 71, Train Loss: 1.9116, Eval Loss: 0.1405\n",
      "Epoch 72, Train Loss: 1.9108, Eval Loss: 0.1406\n",
      "Epoch 73, Train Loss: 1.9102, Eval Loss: 0.1405\n",
      "Epoch 74, Train Loss: 1.9093, Eval Loss: 0.1411\n",
      "Epoch 75, Train Loss: 1.9089, Eval Loss: 0.1400\n",
      "Epoch 76, Train Loss: 1.9082, Eval Loss: 0.1395\n",
      "Epoch 77, Train Loss: 1.9074, Eval Loss: 0.1390\n",
      "Epoch 78, Train Loss: 1.9067, Eval Loss: 0.1386\n",
      "Epoch 79, Train Loss: 1.9063, Eval Loss: 0.1381\n",
      "Epoch 80, Train Loss: 1.9058, Eval Loss: 0.1390\n",
      "Epoch 81, Train Loss: 1.9050, Eval Loss: 0.1375\n",
      "Epoch 82, Train Loss: 1.9047, Eval Loss: 0.1373\n",
      "Epoch 83, Train Loss: 1.9039, Eval Loss: 0.1373\n",
      "Epoch 84, Train Loss: 1.9037, Eval Loss: 0.1374\n",
      "Epoch 85, Train Loss: 1.9029, Eval Loss: 0.1372\n",
      "Epoch 86, Train Loss: 1.9025, Eval Loss: 0.1364\n",
      "Epoch 87, Train Loss: 1.9019, Eval Loss: 0.1366\n",
      "Epoch 88, Train Loss: 1.9014, Eval Loss: 0.1364\n",
      "Epoch 89, Train Loss: 1.9011, Eval Loss: 0.1361\n",
      "Epoch 90, Train Loss: 1.9005, Eval Loss: 0.1355\n",
      "Epoch 91, Train Loss: 1.8998, Eval Loss: 0.1361\n",
      "Epoch 92, Train Loss: 1.8993, Eval Loss: 0.1350\n",
      "Epoch 93, Train Loss: 1.8990, Eval Loss: 0.1353\n",
      "Epoch 94, Train Loss: 1.8986, Eval Loss: 0.1350\n",
      "Epoch 95, Train Loss: 1.8980, Eval Loss: 0.1340\n",
      "Epoch 96, Train Loss: 1.8974, Eval Loss: 0.1348\n",
      "Epoch 97, Train Loss: 1.8970, Eval Loss: 0.1337\n",
      "Epoch 98, Train Loss: 1.8966, Eval Loss: 0.1336\n",
      "Epoch 99, Train Loss: 1.8962, Eval Loss: 0.1348\n",
      "Epoch 100, Train Loss: 1.8958, Eval Loss: 0.1340\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, eval_loader, criterion, optimizer, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1249\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, criterion):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    model.load_state_dict(torch.load('fnoPINO_best_model.pth'))\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_inputs = []\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Move data to CPU for saving\n",
    "            all_inputs.append(inputs.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'Test Loss: {test_loss:.4f}')\n",
    "    \n",
    "    all_inputs = np.concatenate(all_inputs, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    \n",
    "    np.save('inputs.npy', all_inputs)\n",
    "    np.save('targets.npy', all_targets)\n",
    "    np.save('preds.npy', all_preds)\n",
    "    \n",
    "    return all_inputs, all_targets, all_preds\n",
    "\n",
    "all_inputs, all_targets, all_preds = test_model(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1249\n"
     ]
    }
   ],
   "source": [
    "all_inputs, all_targets, all_preds = test_model(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# print(all_preds.shape)\n",
    "# sample_index = np.random.randint(0, 100)\n",
    "\n",
    "# sample_preds = all_preds[sample_index]\n",
    "# sample_inputs = all_inputs[sample_index]\n",
    "# sample_targets = all_targets[sample_index]\n",
    "\n",
    "# fig, axes = plt.subplots(3, 10, figsize=(20, 6))\n",
    "\n",
    "# for i, ax in enumerate(axes[0]):\n",
    "#     ax.imshow(sample_inputs[i, 0, :, :], cmap='jet')\n",
    "#     ax.axis('off')\n",
    "#     ax.set_title(f'Input {i+1}')\n",
    "\n",
    "\n",
    "# for i, ax in enumerate(axes[1]):\n",
    "#     ax.imshow(sample_targets[i, 0, :, :], cmap='jet')\n",
    "#     ax.axis('off')\n",
    "#     ax.set_title(f'Target {i+1}')\n",
    "\n",
    "# for i, ax in enumerate(axes[2]):\n",
    "#     ax.imshow(sample_preds[i, 0, :, :], cmap='jet')\n",
    "#     ax.axis('off')\n",
    "#     ax.set_title(f'Pred {i+1}')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
